{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_out(name, obj, default_path='./sub_result/'):\n",
    "    pickle_out = open(default_path + name + '.pickle','wb')\n",
    "    pickle.dump(obj, pickle_out)\n",
    "    pickle_out.close()\n",
    "    return\n",
    "\n",
    "def pickle_in(name, default_path='./sub_result/'):\n",
    "    pickle_in = open(default_path + name + '.pickle','rb')\n",
    "    obj = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "babelnet_dict = pickle_in('babelnet_dict')\n",
    "merged_text_dict = pickle_in('merged_text_dict')\n",
    "desc_dict = pickle_in('desc_dict')\n",
    "tags_dict = pickle_in('tags_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_list = []\n",
    "for key in list(babelnet_dict.keys()):\n",
    "    # desc and tags\n",
    "    keyword_or_tag_relation = None\n",
    "    if key in tags_dict:\n",
    "        tags = tags_dict[key]\n",
    "        desc_end = desc_dict[key][-10::]\n",
    "        desc_end_idx = merged_text_dict[key].find(desc_end) + 9\n",
    "        data_list = babelnet_dict[key].json()\n",
    "        for item in data_list:\n",
    "            char_start = item['charFragment']['start']\n",
    "            char_end = item['charFragment']['end']\n",
    "            word = merged_text_dict[key][char_start:char_end+1]\n",
    "            if char_end <= desc_end_idx: \n",
    "                keyword_or_tag_relation = 'hasKeyword'\n",
    "            else:\n",
    "                keyword_or_tag_relation = 'hasTag'\n",
    "            DBpediaURL = item['DBpediaURL']\n",
    "            BabelNetURL = item['BabelNetURL']\n",
    "            triples_list.append(((key, keyword_or_tag_relation, word), \n",
    "                                (DBpediaURL, BabelNetURL)))\n",
    "    else:\n",
    "        data_list = babelnet_dict[key].json()\n",
    "        for item in data_list:\n",
    "            char_start = item['charFragment']['start']\n",
    "            char_end = item['charFragment']['end']\n",
    "            word = merged_text_dict[key][char_start:char_end+1]\n",
    "            keyword_or_tag_relation = 'hasKeyword'\n",
    "            DBpediaURL = item['DBpediaURL']\n",
    "            BabelNetURL = item['BabelNetURL']\n",
    "            triples_list.append(((key, keyword_or_tag_relation, word), \n",
    "                                (DBpediaURL, BabelNetURL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out('triples_list', triples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976295"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = []\n",
    "for item in triples_list:\n",
    "    word_set.append((item[0][2], item[1]))\n",
    "    \n",
    "word_set = list(set(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out('word_set', word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84590"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'relation'\n",
    "with open('triple_files/dbpedia_relation.txt', 'w') as f:\n",
    "    for item in word_set:\n",
    "        word = item[0]\n",
    "        dbpedia_url = item[1][0]\n",
    "        if len(dbpedia_url) > 0:\n",
    "            dbpedia_entity = dbpedia_url.split('/')[-1]\n",
    "            f.write(word + '\\t' + p + '\\t' + dbpedia_entity + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a Lemma from a \"<word>.<pos>.<number>.<lemma>\" string where:\n",
    "<word> is the morphological stem identifying the synset\n",
    "<pos> is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
    "<number> is the sense number, counting from 0.\n",
    "<lemma> is the morphological form of interest\n",
    "\n",
    "Note that <word> and <lemma> can be different, e.g. the Synset\n",
    "'salt.n.03' has the Lemmas 'salt.n.03.salt', 'salt.n.03.saltiness' and\n",
    "'salt.n.03.salinity'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_relation = 'synset_member'\n",
    "hyperonym_relation = 'hypernym'\n",
    "wordnet_triples_list = []\n",
    "# phase 1\n",
    "for word_set_item in word_set:\n",
    "    word = word_set_item[0]\n",
    "    synsets = wn.synsets(word)\n",
    "    for synset_mem in synsets:\n",
    "        synset_name = synset_mem.name()\n",
    "        wordnet_triples_list.append((word, synset_relation, synset_name))\n",
    "        \n",
    "        # phase 2\n",
    "        hypernym_set = synset_mem.hypernyms()\n",
    "        for hypernym_mem in hypernym_set:\n",
    "            hypernym_name = hypernym_mem.name()\n",
    "            wordnet_triples_list.append((synset_name, hyperonym_relation, hypernym_name))\n",
    "            \n",
    "            # phase 3\n",
    "            hypernym_set_d2 = hypernym_mem.hypernyms()\n",
    "            for hypernym_mem_d2 in hypernym_set_d2:\n",
    "                hypernym_name_d2 = hypernym_mem_d2.name()\n",
    "                wordnet_triples_list.append((hypernym_name, hyperonym_relation, hypernym_name_d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./triple_files/wordnet_relation.txt', 'w') as f:\n",
    "    for s, p, o in wordnet_triples_list:\n",
    "        f.write(s + '\\t' + p + '\\t' + o + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aisle', 'synset_member', 'aisle.n.01')"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_triples_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating the triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastfm_dataset = '/Users/ian/Documents/GitHub/dsci558/project/dataset/KGRec-dataset/'\n",
    "user_interaction_file = 'implicit_lf_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lastfm_dataset + user_interaction_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    with open('./triple_files/interaction.txt', 'w') as wf:\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            user = 'user_' + items[0]\n",
    "            relation = 'listened'\n",
    "            song = 'song_' + items[1]\n",
    "            wf.write(user + '\\t' + relation + '\\t' + song + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./triple_files/tag_keyword_relation.txt', 'w') as f:\n",
    "    for item in triples_list:\n",
    "        song = 'song_' + str(item[0][0])\n",
    "        relation = item[0][1]\n",
    "        obj = item[0][2]\n",
    "        f.write(song + '\\t' + relation + '\\t' + obj + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbpedia = pd.read_json('dbpedia_sub_broader.jl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./triple_files/dbpedia_sub_broader.txt', 'w') as f:\n",
    "    for s, p, o in df_dbpedia.values:\n",
    "        f.write(s + '\\t' + p + '\\t' + o + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbpedia_relation = pd.read_csv('triple_files/dbpedia_relation.txt', \n",
    "                         names=['s', 'p', 'o'], delimiter='\\t')\n",
    "\n",
    "df_dbpedia_sub_broader = pd.read_csv('triple_files/dbpedia_sub_broader.txt', \n",
    "                         names=['s', 'p', 'o'], delimiter='\\t')\n",
    "\n",
    "df_interaction = pd.read_csv('triple_files/interaction.txt', \n",
    "                         names=['s', 'p', 'o'], delimiter='\\t')\n",
    "\n",
    "df_tag_keyword_relation = pd.read_csv('triple_files/tag_keyword_relation.txt', \n",
    "                         names=['s', 'p', 'o'], delimiter='\\t')\n",
    "\n",
    "df_wordnet_relation = pd.read_csv('triple_files/wordnet_relation.txt', \n",
    "                         names=['s', 'p', 'o'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_dbpedia_relation, df_dbpedia_sub_broader,\n",
    "                       df_interaction, df_tag_keyword_relation,\n",
    "                       df_wordnet_relation], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_set = []\n",
    "for s, p, o in df_merged.values:\n",
    "    container_set.append((s, p, o))\n",
    "container_set = set(container_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kg = pd.DataFrame(container_set, columns=['s', 'p', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kg = df_kg[~df_kg['s'].isnull()]\n",
    "df_kg = df_kg[~df_kg['o'].isnull()]\n",
    "df_kg = df_kg[~(df_kg['p']=='Name')] # 1 non-relevant record removing.\n",
    "df_kg = df_kg.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./final_kg_files/kg_triples.txt', 'w') as f:\n",
    "    for (s, p, o) in zip(df_kg['s'], df_kg['p'], df_kg['o']):\n",
    "        f.write(str(s) + '\\t' + str(p) + '\\t' + str(o) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
